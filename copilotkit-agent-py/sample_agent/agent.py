import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, ToolMessage, BaseMessage, ChatMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool
from pydantic import SecretStr
# NOTE: you must use langchain-core >= 0.3 with Pydantic v2
from pydantic import BaseModel
from typing import Annotated, List, Dict, Any, Optional, Literal
from langgraph.graph.message import add_messages
from operator import add

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph
from langgraph.prebuilt import tools_condition
from copilotkit import CopilotKitState
from copilotkit.langgraph import copilotkit_customize_config

# æ·»åŠ  Astro-Insight è·¯å¾„
astro_insight_path = Path(__file__).resolve().parents[2] / "Astro-Insight-0913v3"
sys.path.insert(0, str(astro_insight_path))
print(f"ğŸ” æ·»åŠ  Astro-Insight è·¯å¾„: {astro_insight_path}")
print(f"ğŸ” è·¯å¾„å­˜åœ¨: {astro_insight_path.exists()}")

# å¯¼å…¥ Astro-Insight çš„èŠ‚ç‚¹å’ŒçŠ¶æ€
from src.graph.nodes import (
    identity_check_command_node,
    qa_agent_command_node,
    task_selector_command_node,
    classification_config_command_node,
    data_retrieval_command_node,
    visualization_command_node,
    multimark_command_node,
    error_recovery_command_node
)
from src.graph.types import AstroAgentState
from src.graph.builder import route_after_identity_check, route_after_task_selection, route_after_error_recovery


class State(CopilotKitState):
    # CopilotKit åŸæœ‰å­—æ®µ
    ask_human: bool
    
    # Astro-Insight å…¼å®¹å­—æ®µ - ä½¿ç”¨ Annotated ç±»å‹é¿å…å¹¶å‘æ›´æ–°å†²çª
    user_type: Optional[Literal["amateur", "professional"]] = "amateur"
    task_type: Optional[Literal["classification", "retrieval", "visualization", "qa"]] = "classification"
    retry_count: int = 0
    error_info: Optional[Dict[str, Any]] = None
    current_step: str = "identity_check"
    next_step: Optional[str] = None
    is_complete: bool = False
    awaiting_user_choice: bool = False
    user_choice: Optional[str] = None
    
    # ä½¿ç”¨æ­£ç¡®çš„ LangGraph çŠ¶æ€å®šä¹‰ï¼Œé¿å…å¹¶å‘æ›´æ–°å†²çª
    # å¯¹äºå¯èƒ½è¢«å¤šä¸ªèŠ‚ç‚¹æ›´æ–°çš„å­—æ®µï¼Œä½¿ç”¨ Annotated ç±»å‹
    qa_response: Annotated[Optional[str], lambda x, y: y if y is not None else x] = None
    response: Optional[str] = None
    final_answer: Annotated[Optional[str], lambda x, y: y if y is not None else x] = None
    generated_code: Optional[str] = None
    execution_result: Optional[Dict[str, Any]] = None
    execution_history: Annotated[List[Dict[str, Any]], add] = []
    node_history: Annotated[List[str], add] = []
    current_node: Optional[str] = None
    timestamp: float = 0.0
    config_data: Dict[str, Any] = {}
    
    # æ·»åŠ  Astro-Insight éœ€è¦çš„é¢å¤–å­—æ®µ
    session_id: str = ""
    user_input: str = ""
    
    # æ·»åŠ  Astro-Insight éœ€è¦çš„å…¶ä»–å­—æ®µ
    visualization_session_id: Optional[str] = None
    visualization_dialogue_state: Optional[Literal["started", "clarifying", "completed", "failed"]] = None
    current_visualization_request: Optional[str] = None
    visualization_turn_count: int = 0
    visualization_max_turns: int = 5
    visualization_dialogue_history: Annotated[List[Dict[str, Any]], add] = []


class RequestAssistance(BaseModel):
    """Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.

    To use this function, relay the user's 'request' so the expert can provide the right guidance.
    """

    request: str


@tool
def search_tool(query: str) -> str:
    """æœç´¢ç›¸å…³ä¿¡æ¯çš„å·¥å…·ã€‚
    
    Args:
        query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        
    Returns:
        str: æœç´¢ç»“æœ
    """
    print(f"ğŸ” æœç´¢å·¥å…·è¢«è°ƒç”¨")
    print(f"   ğŸ“ æœç´¢æŸ¥è¯¢: '{query}'")
    
    # ç®€åŒ–çš„æœç´¢å·¥å…·ï¼Œè¿”å›å›ºå®šçš„ç»“æœ
    result = f"å…³äº '{query}' çš„æœç´¢ç»“æœï¼šè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„æœç´¢ç»“æœã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè¿æ¥åˆ°çœŸå®çš„æœç´¢APIã€‚"
    
    print(f"   ğŸ“Š æœç´¢ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
    print(f"   âœ… æœç´¢å·¥å…·æ‰§è¡Œå®Œæˆ")
    
    return result

tools = [search_tool]
print(f"ğŸ› ï¸ åˆå§‹åŒ–å·¥å…·åˆ—è¡¨ï¼Œå…± {len(tools)} ä¸ªå·¥å…·")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")

# ä¼˜å…ˆä½¿ç”¨ OPENROUTER_API_KEYï¼›æ²¡æœ‰åˆ™å›è½åˆ° OPENAI_API_KEY
api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
if api_key is None:
    raise ValueError("è¯·åœ¨ .env ä¸­è®¾ç½® OPENROUTER_API_KEY æˆ– OPENAI_API_KEY")

base_url = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
model = os.getenv("OPENROUTER_MODEL", "google/gemini-2.5-flash")

print("ğŸ”§ Agent åˆå§‹åŒ–ä¸­...")
print(f"   ğŸ¯ ä½¿ç”¨æ¨¡å‹: {model}")
print(f"   ğŸ”— API ç«¯ç‚¹: {base_url}")
print(f"   ğŸ”‘ API å¯†é’¥: {api_key[:10] if api_key else 'None'}...")

# ä½¿ç”¨ ChatOpenAI æŒ‡å‘ OpenRouter
llm = ChatOpenAI(
    model=model,
    api_key=SecretStr(api_key),
    base_url=base_url,
    temperature=0.7,
    timeout=60,
    max_tokens=1000,  # é™åˆ¶æœ€å¤§ token æ•°
)
print("âœ… LLM åˆå§‹åŒ–å®Œæˆ")

# We can bind the llm to a tool definition, a pydantic model, or a json schema
llm_with_tools = llm.bind_tools(tools + [RequestAssistance])
print(f"ğŸ› ï¸ å·¥å…·ç»‘å®šå®Œæˆï¼Œå…± {len(tools + [RequestAssistance])} ä¸ªå·¥å…·")


def chatbot(state: State, config: RunnableConfig):
    print("ğŸ¤– Chatbot å‡½æ•°è¢«è°ƒç”¨")
    print(f"   ğŸ“¥ æ”¶åˆ°æ¶ˆæ¯æ•°é‡: {len(state.get('messages', []))}")
    if state.get('messages'):
        last_message = state['messages'][-1]
        print(f"   ğŸ’¬ æœ€æ–°æ¶ˆæ¯: {last_message.content[:100]}...")
        print(f"   ğŸ“‹ æ¶ˆæ¯ç±»å‹: {type(last_message).__name__}")
    
    # ä»æœ€æ–°æ¶ˆæ¯ä¸­æå–ç”¨æˆ·è¾“å…¥
    user_input = ""
    if state.get('messages'):
        user_input = state['messages'][-1].content
        print(f"   ğŸ“ æå–çš„ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
    
    # æ‰€æœ‰é—®ç­”éƒ½ç›´æ¥è·³è½¬åˆ° Astro-Insight çš„ Qwen æ¨¡å‹
    if user_input:
        print("   ğŸ¯ æ‰€æœ‰é—®ç­”éƒ½äº¤ç”± Astro-Insight çš„ Qwen æ¨¡å‹å¤„ç†")
        # è¿”å›ä¸€ä¸ªç®€å•çš„ç¡®è®¤æ¶ˆæ¯ï¼Œè®© Astro-Insight å¤„ç†
        response = AIMessage(content="å¥½çš„ï¼Œæˆ‘æ¥å¸®æ‚¨å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚")
        return {"messages": [response], "ask_human": False, "user_input": user_input}
    
    config = copilotkit_customize_config(config, emit_tool_calls="RequestAssistance")
    print("   ğŸ”§ CopilotKit é…ç½®å·²å®šåˆ¶")
    
    print("   ğŸš€ è°ƒç”¨ LLM...")
    response = llm_with_tools.invoke(state["messages"], config=config)
    print(f"   ğŸ“¤ LLM å“åº”ç±»å‹: {type(response).__name__}")
    print(f"   ğŸ“ LLM å“åº”å†…å®¹: {response.content[:100] if response.content else 'None'}...")
    
    ask_human = False
    
    # Check if response is AIMessage and has additional_kwargs with tool_calls
    if isinstance(response, AIMessage) and hasattr(response, "additional_kwargs"):
        tool_calls = response.additional_kwargs.get("tool_calls", [])
        print(f"   ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {len(tool_calls)} ä¸ª")
        
        if tool_calls:
            for i, tool_call in enumerate(tool_calls):
                tool_name = tool_call.get("function", {}).get("name", "unknown")
                print(f"      ğŸ› ï¸ å·¥å…· {i+1}: {tool_name}")
                
                if tool_call.get("name") == RequestAssistance.__name__:
                    ask_human = True
                    print("      ğŸ‘¤ è§¦å‘äººå·¥ååŠ©è¯·æ±‚")
    
    print(f"   ğŸ¯ è¿”å›çŠ¶æ€: ask_human = {ask_human}")
    print("âœ… Chatbot å‡½æ•°æ‰§è¡Œå®Œæˆ")
    
    return {"messages": [response], "ask_human": ask_human, "user_input": user_input}


graph_builder = StateGraph(State)

print("ğŸ”— æ„å»º LangGraph...")
graph_builder.add_node("chatbot", chatbot)
print("   âœ… æ·»åŠ  chatbot èŠ‚ç‚¹")

# æ·»åŠ  Astro-Insight èŠ‚ç‚¹
graph_builder.add_node("identity_check", identity_check_command_node)
print("   âœ… æ·»åŠ  identity_check èŠ‚ç‚¹")

graph_builder.add_node("qa_agent", qa_agent_command_node)
print("   âœ… æ·»åŠ  qa_agent èŠ‚ç‚¹")

graph_builder.add_node("task_selector", task_selector_command_node)
print("   âœ… æ·»åŠ  task_selector èŠ‚ç‚¹")

graph_builder.add_node("classification_config", classification_config_command_node)
print("   âœ… æ·»åŠ  classification_config èŠ‚ç‚¹")

graph_builder.add_node("data_retrieval", data_retrieval_command_node)
print("   âœ… æ·»åŠ  data_retrieval èŠ‚ç‚¹")

graph_builder.add_node("visualization", visualization_command_node)
print("   âœ… æ·»åŠ  visualization èŠ‚ç‚¹")

graph_builder.add_node("multimark", multimark_command_node)
print("   âœ… æ·»åŠ  multimark èŠ‚ç‚¹")

graph_builder.add_node("error_recovery", error_recovery_command_node)
print("   âœ… æ·»åŠ  error_recovery èŠ‚ç‚¹")

# åˆ é™¤ tools èŠ‚ç‚¹ï¼Œå› ä¸ºç°åœ¨ç›´æ¥è·³è½¬åˆ° Astro-Insight


# åˆ é™¤ä¸å†éœ€è¦çš„å·¥å…·å’Œäººå·¥èŠ‚ç‚¹å‡½æ•°


# åˆ é™¤ human èŠ‚ç‚¹ï¼Œå› ä¸ºç°åœ¨ç›´æ¥è·³è½¬åˆ° Astro-Insight


def select_next_node(state: State):
    print("ğŸ”€ é€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹...")
    print(f"   ğŸ” ask_human çŠ¶æ€: {state.get('ask_human', False)}")
    
    # ç›´æ¥è·³è½¬åˆ° Astro-Insight çš„ identity_check èŠ‚ç‚¹
    print("   ğŸš€ ç›´æ¥è·³è½¬åˆ° Astro-Insight identity_check...")
    return "identity_check"


print("ğŸ”— æ·»åŠ å›¾çš„è¾¹å’Œæ¡ä»¶è·¯ç”±...")
# ä» chatbot ç›´æ¥è·³è½¬åˆ° identity_check
graph_builder.add_conditional_edges(
    "chatbot",
    select_next_node,
    {"identity_check": "identity_check"},
)
print("   âœ… æ·»åŠ  chatbot åˆ° identity_check çš„æ¡ä»¶è¾¹")

# ä» identity_check åˆ°å…¶ä»– Astro-Insight èŠ‚ç‚¹
graph_builder.add_conditional_edges(
    "identity_check",
    route_after_identity_check,
    {
        "qa_agent": "qa_agent",
        "task_selector": "task_selector",
        "error_recovery": "error_recovery"
    }
)
print("   âœ… æ·»åŠ  identity_check çš„æ¡ä»¶è¾¹")

# ä» task_selector åˆ°ä¸“ä¸šåŠŸèƒ½èŠ‚ç‚¹
graph_builder.add_conditional_edges(
    "task_selector",
    route_after_task_selection,
    {
        "classification_config": "classification_config",
        "data_retrieval": "data_retrieval",
        "visualization": "visualization",
        "multimark": "multimark",
        "error_recovery": "error_recovery"
    }
)
print("   âœ… æ·»åŠ  task_selector çš„æ¡ä»¶è¾¹")

# ä» error_recovery åˆ°ç»“æŸ
graph_builder.add_conditional_edges(
    "error_recovery",
    route_after_error_recovery,
    {
        "__end__": "__end__"
    }
)
print("   âœ… æ·»åŠ  error_recovery çš„æ¡ä»¶è¾¹")

# æ‰€æœ‰ä¸“ä¸šåŠŸèƒ½èŠ‚ç‚¹éƒ½ç›´æ¥ç»“æŸ
graph_builder.add_edge("qa_agent", "__end__")
print("   âœ… æ·»åŠ  qa_agent -> __end__ è¾¹")

graph_builder.add_edge("classification_config", "__end__")
print("   âœ… æ·»åŠ  classification_config -> __end__ è¾¹")

graph_builder.add_edge("data_retrieval", "__end__")
print("   âœ… æ·»åŠ  data_retrieval -> __end__ è¾¹")

graph_builder.add_edge("visualization", "__end__")
print("   âœ… æ·»åŠ  visualization -> __end__ è¾¹")

graph_builder.add_edge("multimark", "__end__")
print("   âœ… æ·»åŠ  multimark -> __end__ è¾¹")

# åˆ é™¤åŸæœ‰çš„å·¥å…·å’Œäººå·¥èŠ‚ç‚¹è¾¹ï¼Œå› ä¸ºç°åœ¨ç›´æ¥è·³è½¬åˆ° Astro-Insight

graph_builder.set_entry_point("chatbot")
print("   âœ… è®¾ç½® chatbot ä¸ºå…¥å£ç‚¹")

memory = MemorySaver()
print("ğŸ’¾ åˆå§‹åŒ–å†…å­˜ä¿å­˜å™¨")

graph = graph_builder.compile(
    checkpointer=memory,
)
print("âœ… LangGraph ç¼–è¯‘å®Œæˆ")
print("ğŸ‰ Agent åˆå§‹åŒ–å®Œæˆï¼")