# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

import pandas as pd
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import os

from .types import DatasetInfo


class DatasetManager:
    """æ•°æ®é›†ç®¡ç†å™¨ - è¯»å–å’Œåˆ†æå¯ç”¨æ•°æ®é›†"""
    
    def __init__(self, dataset_dir: str = None):
        # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„æ•°æ®é›†ç›®å½•
        if dataset_dir is None:
            dataset_dir = os.environ.get("ASTRO_DATASET_DIR", "dataset")
        
        self.dataset_dir = Path(dataset_dir)
        self.available_datasets: List[DatasetInfo] = []
        self._load_available_datasets()
    
    def _load_available_datasets(self):
        """åŠ è½½æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        if not self.dataset_dir.exists():
            print(f"âš ï¸ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {self.dataset_dir}")
            return
        
        # æ‰«ææ•°æ®é›†ç›®å½•
        for dataset_path in self.dataset_dir.rglob("*.csv"):
            try:
                dataset_info = self._analyze_dataset(dataset_path)
                if dataset_info:
                    self.available_datasets.append(dataset_info)
            except Exception as e:
                print(f"âŒ åˆ†ææ•°æ®é›†å¤±è´¥ {dataset_path}: {e}")
        
        print(f"ğŸ“Š å‘ç° {len(self.available_datasets)} ä¸ªæ•°æ®é›†")
    
    def _analyze_dataset(self, file_path: Path) -> Optional[DatasetInfo]:
        """åˆ†æå•ä¸ªæ•°æ®é›†æ–‡ä»¶"""
        try:
            # è¯»å–æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
            df = pd.read_csv(file_path, nrows=1000)  # åªè¯»å‰1000è¡Œè¿›è¡Œå¿«é€Ÿåˆ†æ
            
            # è·å–åˆ—åå’Œæ•°æ®ç±»å‹
            columns = df.columns.tolist()
            data_types = df.dtypes.astype(str).to_dict()
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            
            # è·å–æ ·æœ¬æ•°æ®ï¼ˆå‰5è¡Œï¼‰
            sample_data = df.head(5).to_dict('records')
            
            # ç”Ÿæˆæè¿°
            description = self._generate_dataset_description(df, file_path)
            
            return DatasetInfo(
                name=file_path.stem,
                path=str(file_path),
                description=description,
                columns=columns,
                size=file_size,
                file_type="csv",
                sample_data=sample_data,
                data_types=data_types
            )
            
        except Exception as e:
            print(f"âŒ åˆ†ææ•°æ®é›†å¤±è´¥ {file_path}: {e}")
            return None
    
    def _generate_dataset_description(self, df: pd.DataFrame, file_path: Path) -> str:
        """ç”Ÿæˆæ•°æ®é›†æè¿°"""
        try:
            # åŸºäºæ–‡ä»¶åå’Œå†…å®¹ç”Ÿæˆæè¿°
            filename = file_path.stem.lower()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤©æ–‡å­¦ç›¸å…³æ•°æ®é›†
            if any(keyword in filename for keyword in ['astro', 'galaxy', 'star', 'sdss', 'cosmic']):
                if 'galaxy' in filename:
                    return "æ˜Ÿç³»åˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…å«æ˜Ÿç³»çš„å½¢æ€å­¦ç‰¹å¾å’Œåˆ†ç±»æ ‡ç­¾"
                elif 'star' in filename:
                    return "æ’æ˜Ÿåˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…å«æ’æ˜Ÿçš„ç‰©ç†ç‰¹å¾å’Œç±»å‹ä¿¡æ¯"
                elif 'sdss' in filename:
                    return "SDSS (æ–¯éš†æ•°å­—å·¡å¤©) æ•°æ®é›†ï¼ŒåŒ…å«å¤©ä½“çš„è§‚æµ‹æ•°æ®"
                else:
                    return "å¤©æ–‡å­¦æ•°æ®é›†ï¼ŒåŒ…å«å¤©ä½“çš„è§‚æµ‹å’Œåˆ†ç±»ä¿¡æ¯"
            
            # åŸºäºåˆ—åæ¨æ–­
            columns_str = ' '.join(df.columns).lower()
            if any(keyword in columns_str for keyword in ['magnitude', 'ra', 'dec', 'redshift', 'galaxy', 'star']):
                return "å¤©æ–‡å­¦æ•°æ®é›†ï¼ŒåŒ…å«å¤©ä½“çš„ä½ç½®ã€äº®åº¦ã€çº¢ç§»ç­‰è§‚æµ‹å‚æ•°"
            
            # é€šç”¨æè¿°
            return f"åŒ…å« {len(df.columns)} ä¸ªç‰¹å¾çš„æ•°æ®é›†ï¼Œå…± {len(df)} è¡Œæ•°æ®"
            
        except Exception:
            return "æ•°æ®é›†ä¿¡æ¯"
    
    def get_available_datasets(self) -> List[DatasetInfo]:
        """è·å–æ‰€æœ‰å¯ç”¨æ•°æ®é›†"""
        return self.available_datasets
    
    def get_dataset_by_name(self, name: str) -> Optional[DatasetInfo]:
        """æ ¹æ®åç§°è·å–æ•°æ®é›†"""
        for dataset in self.available_datasets:
            if dataset.name == name:
                return dataset
        return None
    
    def get_dataset_summary(self) -> str:
        """è·å–æ•°æ®é›†æ‘˜è¦ä¿¡æ¯ï¼ˆç”¨äºLLMé€‰æ‹©ï¼‰"""
        if not self.available_datasets:
            return "æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†"
        
        summary = "å¯ç”¨æ•°æ®é›†åˆ—è¡¨ï¼š\n\n"
        for i, dataset in enumerate(self.available_datasets, 1):
            summary += f"{i}. **{dataset.name}**\n"
            summary += f"   - æè¿°: {dataset.description}\n"
            summary += f"   - åˆ—æ•°: {len(dataset.columns)}\n"
            summary += f"   - æ–‡ä»¶å¤§å°: {self._format_file_size(dataset.size)}\n"
            summary += f"   - ä¸»è¦åˆ—: {', '.join(dataset.columns[:5])}{'...' if len(dataset.columns) > 5 else ''}\n\n"
        
        return summary
    
    def get_detailed_dataset_info(self, dataset_name: str) -> Optional[str]:
        """è·å–æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯"""
        dataset = self.get_dataset_by_name(dataset_name)
        if not dataset:
            return None
        
        info = f"**æ•°æ®é›†: {dataset.name}**\n\n"
        info += f"**æè¿°**: {dataset.description}\n\n"
        info += f"**æ–‡ä»¶è·¯å¾„**: {dataset.path}\n"
        info += f"**æ–‡ä»¶å¤§å°**: {self._format_file_size(dataset.size)}\n"
        info += f"**æ•°æ®åˆ—æ•°**: {len(dataset.columns)}\n\n"
        
        info += "**æ•°æ®åˆ—è¯¦æƒ…**:\n"
        for col in dataset.columns:
            data_type = dataset.data_types.get(col, 'unknown')
            info += f"- {col} ({data_type})\n"
        
        if dataset.sample_data:
            info += "\n**æ ·æœ¬æ•°æ®** (å‰3è¡Œ):\n"
            for i, row in enumerate(dataset.sample_data[:3]):
                info += f"è¡Œ {i+1}: {dict(list(row.items())[:3])}...\n"
        
        return info
    
    def _format_file_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        else:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
    
    def suggest_dataset_for_requirement(self, user_requirement: str) -> Optional[DatasetInfo]:
        """æ ¹æ®ç”¨æˆ·éœ€æ±‚å»ºè®®æœ€åˆé€‚çš„æ•°æ®é›†"""
        if not self.available_datasets:
            return None
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        requirement_lower = user_requirement.lower()
        
        # æ£€æŸ¥æ˜¯å¦æ˜ç¡®æåˆ°äº†æ•°æ®é›†åç§°
        for dataset in self.available_datasets:
            if dataset.name.lower() in requirement_lower:
                return dataset
        
        # åŸºäºéœ€æ±‚å†…å®¹åŒ¹é…
        for dataset in self.available_datasets:
            dataset_text = f"{dataset.name} {dataset.description} {' '.join(dataset.columns)}".lower()
            
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            if any(keyword in requirement_lower for keyword in ['galaxy', 'æ˜Ÿç³»']) and 'galaxy' in dataset_text:
                return dataset
            elif any(keyword in requirement_lower for keyword in ['star', 'æ’æ˜Ÿ']) and 'star' in dataset_text:
                return dataset
            elif any(keyword in requirement_lower for keyword in ['sdss']) and 'sdss' in dataset_text:
                return dataset
        
        # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªæ•°æ®é›†
        return self.available_datasets[0] if self.available_datasets else None
    
    def refresh_datasets(self):
        """åˆ·æ–°æ•°æ®é›†åˆ—è¡¨"""
        self.available_datasets.clear()
        self._load_available_datasets()
